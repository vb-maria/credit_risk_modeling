# -*- coding: utf-8 -*-
"""vixidx.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ykw3NZ7OZARfJQXAAddQ1J1uADu9qyxX

# Importing
"""

import pandas as pd
import sys
import numpy as np
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 999)

from google.colab import drive
drive.mount('/content/drive')

# Append the directory to your python path using sys
sys.path.append('/content/drive/MyDrive/BEFORE 2024/Project IDX')
# Import the modules
from utils.essential import *
from utils.preparation import *
from utils.preparation_visualization import *
from utils.preparation_preprocessing import *

path_df_original = '/content/drive/MyDrive/BEFORE 2024/Project IDX/loan_data_2007_2014.csv' # - (Ensure path is correct)
#Original Dataframe definition
df_original = pd.read_csv(path_df_original, low_memory=False)

"""# Data Understanding

"""

# Inspection of the first entries of the dataset
df_original.head(5)

total_entries = df_original.shape[0]
total_columns = df_original.shape[1]
# Count number of columns
print('The number of features in the original dataset is', total_columns)
# Count number of entres
print('The number of records in the original dataset is', total_entries)

duplicates = df_original.duplicated()
num_duplicates = duplicates.sum()
print("Number of duplicated rows:", num_duplicates)

missing_values = df_original.isnull().sum()
total_values = df_original.size
total_missing = missing_values.sum()
percentage_missing_of_total = round((total_missing / total_values) * 100)

print("Number of missing values:", total_missing)
print(f"Percentage of missing values relative to total values: {percentage_missing_of_total}%")

def select_null_constant_columns(df, null_threshold=1.0, constant_threshold=0.0):
    # Identify fully null columns
    null_columns = df.columns[df.isnull().all()]

    # Identify constant columns
    constant_columns = df.columns[df.nunique() == 1]

    # Apply thresholds for null and constant columns
    null_columns = [col for col in null_columns if df[col].isnull().mean() >= null_threshold]
    constant_columns = [col for col in constant_columns if df[col].nunique() == 1 and df[col].notnull().all()]

    return null_columns, constant_columns

table_title = ['Unnecessary columns']
column_titles = ['Fully null columns:', 'Constant columns:']
print_table(table_title, column_titles, null_cols, constant_cols)

unusable_cols = null_cols + constant_cols
unusable_cols

"""We can check the rest of the columns as a sample of the most complete row, with the type and the value of its numerical variables and categorical features."""

filtered_cols = exclude_elements(df_original.columns, unusable_cols)
df_filtered  = df_original[filtered_cols]
categorical_cols, numerical_cols = select_columns_by_type(df_filtered)
original_vt_valid_row = get_valid_row(df_filtered)
original_vt_cat = get_data_type_by_row(df_filtered, original_vt_valid_row, categorical_cols)
original_vt_num = get_data_type_by_row(df_filtered, original_vt_valid_row, numerical_cols)

table_title = ['Sample of the most complete row']
column_titles = ['Categorical', 'Numerical']
print_table(table_title, column_titles, original_vt_cat, original_vt_num )

"""Looking at the row sample value of each feature and and the feature type, there are mistyped features that will be correctly typed during preprocessing:"""

# should be data typed as a numerical feature (represent time lengths)
to_numerical = ["emp_length", "term"]
# should be data typed as a date, a numerical(time) feature.
to_temporal = ['earliest_cr_line', 'issue_d', 'last_pymnt_d', 'last_credit_pull_d', 'next_pymnt_d']

"""### Target Variable Inspection:
- The intended target of the model is the variable loan_status.
"""

# Unique values of Target Variable
df_original['loan_status'].value_counts()

"""However this variable has 9 different categories. A new target will be created based on loan_satus by mapping the categories into a probability of default of 0 or 1, which will turn this into a binary classification problem.

# Data preparation
"""

# Copy to prepare the data without modifying the original df
df_basic = df_original.copy()

"""#### Target Variable Transformation"""

(df_basic['loan_status'].value_counts() / len(df_basic)) * 100

custom_mapping = {
    'Current': 0,
    'Fully Paid': 0,
    'In Grace Period': 0,
    'Does not meet the credit policy. Status:Fully Paid': 0,
    'Late (16-30 days)': 0,
    'Charged Off': 1,
    'Default': 1,
    'Does not meet the credit policy. Status:Charged Off': 1,
    'Late (31-120 days)': 1,
}

# Ensure the 'loan_status' column is of string type and strip any leading/trailing whitespace
df_basic['loan_status'] = df_basic['loan_status'].astype(str).str.strip()

# Use the custom mapping to create the 'default_likelihood' column
df_basic['default_likelihood'] = df_basic['loan_status'].map(custom_mapping)

# Check the unique values and data type of the 'default_likelihood' column
print(df_basic['default_likelihood'].value_counts())
print(df_basic['default_likelihood'].dtype)

"""#### Non-Informative / Unnecessary  Data Elimination"""

unused_target = "loan_status"
df_basic = df_basic.drop(columns=[unused_target])

# Remove fully null and constant columns (if any)
df_basic = df_basic.drop(columns=unusable_cols)

"""#### Data Formatting and Typing"""

def column_unique_values(df, column_name):
    unique_values = df[column_name].unique()
    return list(unique_values)

##### To numerical
import pandas as pd
import numpy as np
import re
def extract_value(value):
    match = re.search(r'(\d+)', value)
    if match:
        return match.group(1)
    else:
        return None
def get_replacement_value(value):
    if pd.isna(value):
        return np.nan
    else:
        return extract_value(value)
print("Columns to format:")
for col in to_numerical:
    col_values = column_unique_values(df_basic, col)
    print(f"{col}: {col_values}")
def map_value(value):
    if col == '< 1 year':
        return 0
    else:
        return get_replacement_value(value)
print("Formatted Columns")
for col in to_numerical:
    df_basic[col] = df_basic[col].apply(map_value)
    col_values = column_unique_values(df_basic, col)
    print(f"{col}: {col_values}")
for col in to_numerical:
    df_basic[col] = df_basic[col].astype('float64')
    data_type = get_data_type(df_basic, col)
    print(f"'{col}' correctly typed as: {data_type}")
df_basic['default_likelihood'] = df_basic['default_likelihood'].astype('int64')
import pandas as pd
from datetime import datetime


def format_and_convert_date(df, columns_to_convert, date_format='%b-%y'):
    for column in columns_to_convert:
        if df[column].dtype != 'datetime64[ns]':
            df[column] = df[column].str.strip()  # Remove leading and trailing whitespaces in values
            df[column] = pd.to_datetime(df[column], format=date_format, errors='coerce')
    return df
date_format = "%b-%y"
df_basic = format_and_convert_date(df_basic, to_temporal, date_format)

for col in to_temporal:
    data_type = get_data_type(df_basic, col)
    print(f"'{col}' correctly typed as: {data_type}")

df_basic[to_temporal].info()

def select_datetime_columns(df):
    datetime_columns = df.select_dtypes(include=['datetime']).columns
    return datetime_columns

datetime_cols = select_datetime_columns(df_basic)

print("Datetime columns:", datetime_cols)

"""# Data Understanding Part 2

## Unnecessary data check
"""

categorical_cols, numerical_cols = select_columns_by_type(df_basic)
categorical = df_basic[categorical_cols]

# High Cardinality Check
total_rows = len(df_basic)
cardinality_threshold = 0.001  # You can adjust this threshold as needed
high_cardinality = []
for col in categorical.columns:
    unique_values = categorical[col].nunique()
    value_counts = categorical[col].value_counts()
    if len(value_counts) / total_rows > cardinality_threshold:
        high_cardinality.append(col)
        print(f"High Cardinality Detected in {col} | {unique_values} different categories")

def find_columns_with_dominant_class(df, threshold=0.8):
    dominant_columns = []

    for column in df.columns:
        value_counts = df[column].value_counts(normalize=True)
        top_class = value_counts.index[0]
        top_class_percentage = value_counts.iloc[0]

        if top_class_percentage >= threshold:
            dominant_columns.append((column, top_class, top_class_percentage))

    return dominant_columns


result = find_columns_with_dominant_class(categorical, threshold=0.85)
for column, top_class, top_class_percentage in result:
    print(f"Column '{column}' has a dominant class '{top_class}' with {top_class_percentage*100:.4f}% of the values.")

df_basic.pymnt_plan.value_counts()

unnecessary_categorical = high_cardinality + ['pymnt_plan','sub_grade']

unnecessary_numerical = ['Unnamed: 0', 'member_id', 'id']

# Calculate the missing percentages and get the data lists
potential_missing_cat = exclude_elements(categorical_cols, unnecessary_categorical)
df_potential_missing_cat = df_basic[potential_missing_cat]
cat_low_col, cat_low_val, cat_high_col, cat_high_val = calculate_missing_percentages(df_potential_missing_cat, 50)
table_title = ["Rest of Categorical Columns by Missing Data %"]
column_titles = ["Low-Moderate % of Missing Values", '0-49%', '', "High % of Missing Values", "50% - 99%"]
print_table(table_title, column_titles, cat_low_col, cat_low_val, '', cat_high_col, cat_high_val)

potential_missing_num = exclude_elements(df_basic[numerical_cols], unnecessary_numerical)
df_potential_missing_num = df_basic[potential_missing_num]
num_low_col, num_low_val, num_high_col, num_high_val = calculate_missing_percentages(df_potential_missing_num, 50)
table_title = ["Numerical by  Columns Data %"]
column_titles = ["Low-Moderate % of Missing Values", '0-49%', '', "High % of Missing Values", "50% - 99%"]
print_table(table_title, column_titles, num_low_col, num_low_val, '', num_high_col, num_high_val)

unnecessary_numerical = join_column_lists(unnecessary_numerical, num_high_col)
print(unnecessary_numerical)

df_potential_missing_temp = df_basic[to_temporal]
num_low_col, num_low_val, num_high_col, num_high_val = calculate_missing_percentages(df_potential_missing_temp, 50)
table_title = ["Datetime by  Columns Data %"]
column_titles = ["Low-Moderate % of Missing Values", '0-49%', '', "High % of Missing Values", "50% - 99%"]
print_table(table_title, column_titles, num_low_col, num_low_val, '', num_high_col, num_high_val)

df_basic = df_basic.drop(columns=['next_pymnt_d','issue_d','last_pymnt_d'])

used_temp = ['earliest_cr_line', 'last_credit_pull_d']

# Extract month information
df_basic['earliest_cr_line_month'] = df_basic['earliest_cr_line'].dt.month
df_basic['last_credit_pull_d_month'] = df_basic['last_credit_pull_d'].dt.month


df_basic = df_basic.drop(columns=used_temp)

"""## Basic EDA"""

# Initialize a dictionary to store the count of unique values
unique_value_counts = {}

for col_name in categorical_cols:
    unique_count = df_basic[col_name].nunique()
    unique_value_counts[col_name] = unique_count
    print(f"Total unique values in column '{col_name}': {unique_count}")

cols_to_plot = ['default_likelihood',
 'initial_list_status',
 'term',
 'home_ownership',
 'verification_status']
custom_functions_list = [
    custom_barplot, custom_barplot, custom_barplot, custom_barplot, custom_barplot
]

withingrid_col_per_function(df_basic, cols_to_plot, custom_functions_list=custom_functions_list, figsize=(20, 10))

def custom_hor_barplot(df, column, ax, custom_palette=None):
    value_counts = df[column].value_counts()
    sns.barplot(x=value_counts.values, y=value_counts.index, data=df, ax=ax, palette=custom_palette)
    ax.set_title(f'Horizontal Bar Plot: {column}')
cols_to_plot = ['purpose','grade', 'addr_state']
custom_function = custom_hor_barplot

withingrid_function_per_cols(df_basic, cols_to_plot, custom_function, figsize=(20, 10))

"""# Data Preparation Part 2
- There is no duplicated data so no treatment is necessary.
- Models robust to outliers will be used during modeling so there is no need to handle outlers

#### Non-Informative / Unnecessary  Data *Elimination*
"""

df_basic = df_basic.drop(columns=unnecessary_categorical)
df_basic = df_basic.drop(columns=unnecessary_numerical)

"""#### Data split"""

# Feature / Label Split
X = df_basic.drop('default_likelihood', axis=1)
y = df_basic['default_likelihood']

# Split the balanced data into training and testing set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
check_feature_consistency(X_train, X_test)

# Performing Imputation
# With Data Split
X_train_Imp, X_test_Imp = impute_data(X_train, split_data=True, X_test=X_test, drop_all=False)  # Imputing rows with missing values
check_feature_consistency(X_train_Imp, X_test_Imp)
numerical_cols, categorical_cols = list_column_types(X_train_Imp)

# Concatenate the training and testing data for one-hot encoding
X_concatenated = pd.concat([X_train_Imp, X_test_Imp])

# Separate numerical columns (excluding datetime columns)
X_categorical = X_concatenated.select_dtypes(exclude=['datetime64','number']).copy()

# Separate numerical columns (excluding datetime columns)
X_numerical = X_concatenated.select_dtypes(include=['datetime64','number']).copy()

# One-hot encode categorical data
X_categorical_encoded = one_hot_encode(X_categorical)

# Scale numerical data
X_numerical_scaled = scale_features(X_numerical, scaler_type='Robust')

# Concatenate the processed data
X_encoded_scaled = pd.concat([X_categorical_encoded, X_numerical_scaled], axis=1)

# Split the encoded data back into training and testing datasets
X_train_ready = X_encoded_scaled.iloc[:len(X_train_Imp)]
X_test_ready = X_encoded_scaled.iloc[len(X_train_Imp):]

check_feature_consistency(X_train_ready, X_test_ready)
numerical_cols, categorical_cols = list_column_types(X_train_ready)

# Apply Random Oversampling
from imblearn.over_sampling import RandomOverSampler
ros = RandomOverSampler(random_state=42)
X_train_oversampled, y_train_oversampled = ros.fit_resample(X_train_ready, y_train)

df_heatmap = X_train_oversampled
plt.figure(figsize=(20, 15))
ax = plt.gca()
corr_matrix = custom_heatmap(df_heatmap, ax)
plt.show()

print("Pairs of features with high correlation between them:")
high_corr_pairs = []
for i in range(len(corr_matrix.columns)):
    for j in range(i):
        if abs(corr_matrix.iloc[i, j]) >= 0.9:
            high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j]))

for pair in high_corr_pairs:
    print(f"{pair[0]} and {pair[1]}")
features_to_drop = set()
for pair in high_corr_pairs:
    features_to_drop.update(pair)
high_corr_features = list(features_to_drop)

drop = ['out_prncp_inv', 'funded_amnt_inv', 'funded_amnt', 'total_rec_prncp', 'total_pymnt_inv']

X_train_oversampled = X_train_oversampled.drop(columns=drop)
X_test_ready = X_test_ready.drop(columns=drop)

"""# Modeling and Evaluation"""

# Define the models
from lightgbm import LGBMClassifier
from xgboost import XGBClassifier


def train_models(X_train, y_train, X_test, y_test, models):
    trained_models = []
    for model, model_name in models:
        print(f'Training {model_name}:')
        model.fit(X_train, y_train)
        print("Finished")
        trained_models.append((model_name, model))
    print("All models have Finished training.")
    return trained_models

def evaluate_models(trained_models, X_test, y_test):
    results = []
    for i, (model_name, _) in enumerate(trained_models):
        model = trained_models[i][1]
        y_pred = model.predict(X_test)
        f1 = f1_score(y_test, y_pred)
        print(f'{model_name} F1 Score: {f1:.2f}')
        results.append((model_name, f1))
    return results

# Define the models
models = [
    (LGBMClassifier(verbosity=-1), 'LightGBM'),
    (XGBClassifier(), 'XGBoost'),
]

print("Training models without tuning:")
var1_trained_models_no_tuning = train_models(X_train_oversampled, y_train_oversampled, X_test_ready, y_test, models)

from sklearn.metrics import f1_score
print("Evaluating models without tuning:")
var1_results_no_tuning = evaluate_models(var1_trained_models_no_tuning, X_test_ready, y_test)

var1_results_no_tuning[0]

var1_results_no_tuning[1]